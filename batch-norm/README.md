# Description 

In this notebook,The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to layers within the network.
It's called batch normalization because during training, we normalize each layer's inputs by using the mean and variance of the values in the current batch.
